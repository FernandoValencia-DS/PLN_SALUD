{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Estudiante\n",
    "### Fernando Valencia"
   ],
   "metadata": {
    "id": "xihi--UYFpkW"
   },
   "id": "xihi--UYFpkW"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# \ud83e\udde0 Entregable 1 \u2014 Fine-Tuning para extracci\u00f3n de entidades en c\u00e1ncer de pulm\u00f3n\n",
    "\n",
    "Este notebook documenta el desarrollo de una herramienta de **extracci\u00f3n de informaci\u00f3n cl\u00ednica autom\u00e1tica** a partir de historias cl\u00ednicas relacionadas con **c\u00e1ncer de pulm\u00f3n**, usando t\u00e9cnicas de **Fine-Tuning** y **Transfer Learning** sobre modelos de lenguaje natural.\n",
    "\n",
    "## \ud83c\udfaf Objetivo\n",
    "\n",
    "Ajustar  un modelo de lenguaje preentrenado sobre un corpus cl\u00ednico anotado con entidades relevantes, de modo que aprenda a reconocer autom\u00e1ticamente conceptos clave en nuevos textos m\u00e9dicos.\n",
    "\n",
    "## \ud83d\uddc3\ufe0f Dataset de entrenamiento\n",
    "\n",
    "Se utiliza un corpus anotado de historias cl\u00ednicas de **c\u00e1ncer de pulm\u00f3n**, disponible en el siguiente enlace:\n",
    "\n",
    "\ud83d\udcce [Corpus Anotado \u2014 Historias Cl\u00ednicas de C\u00e1ncer de Pulm\u00f3n](https://drive.google.com/drive/folders/1DBnlco-vBtEXktGQCR7QR4vwVv8JMw46)\n",
    "\n",
    "Este dataset contiene frases cl\u00ednicas con anotaciones en formato BIO (Begin-Inside-Outside), asociadas a diferentes tipos de entidades cl\u00ednicas, como diagn\u00f3sticos, procedimientos, hallazgos, entre otros.\n",
    "\n",
    "## \ud83d\udd0d Modelo Utilizado\n",
    "\n",
    "Se us\u00f3 el modelo multiling\u00fce `xlm-roberta-base`. Este modelo fue seleccionado por su robustez en tareas multiling\u00fces y su capacidad para adaptarse a datos biom\u00e9dicos en espa\u00f1ol mediante fine-tuning.\n"
   ],
   "metadata": {
    "id": "WGOpg6oaPT-U"
   },
   "id": "WGOpg6oaPT-U"
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IdXmtq032Olz",
    "outputId": "f3081bed-faf3-44b1-a9bc-1d846f9436a9"
   },
   "id": "IdXmtq032Olz",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ed349a-ec06-4559-83dd-a20d0566fc4a",
   "metadata": {
    "scrolled": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "77ed349a-ec06-4559-83dd-a20d0566fc4a",
    "outputId": "78ceb40c-6794-4d21-84a3-23d2f8124cbb",
    "collapsed": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from seqeval) (2.0.2)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.6.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16162 sha256=be970adcf0570213bbf1d7d80c5b915b09a882c29c271f71d38ce73cdff54b76\n",
      "  Stored in directory: /root/.cache/pip/wheels/bc/92/f0/243288f899c2eacdfa8c5f9aede4c71a9bad0ee26a01dc5ead\n",
      "Successfully built seqeval\n",
      "Installing collected packages: seqeval\n",
      "Successfully installed seqeval-1.2.2\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.4-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.5.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading evaluate-0.4.4-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fsspec, datasets, evaluate\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.14.4\n",
      "    Uninstalling datasets-2.14.4:\n",
      "      Successfully uninstalled datasets-2.14.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-3.6.0 evaluate-0.4.4 fsspec-2025.3.0\n",
      "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.0)\n",
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.33.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.6.15)\n",
      "Downloading huggingface_hub-0.33.1-py3-none-any.whl (515 kB)\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m515.4/515.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: huggingface_hub\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface-hub 0.33.0\n",
      "    Uninstalling huggingface-hub-0.33.0:\n",
      "      Successfully uninstalled huggingface-hub-0.33.0\n",
      "Successfully installed huggingface_hub-0.33.1\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets transformers\n",
    "!pip install seqeval\n",
    "! pip install -U datasets evaluate\n",
    "!pip install -U huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ff1331-c6de-41c1-a1b1-0452bf485f7d",
   "metadata": {
    "id": "68ff1331-c6de-41c1-a1b1-0452bf485f7d"
   },
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset, Features, Sequence, Value, ClassLabel\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# \ud83d\udcc4Rutas de Archivos\n",
    "\n",
    "Se definen las rutas de los tres archivos _(train, test y valid)_  con los que se har\u00e1 el ajuste del modelo."
   ],
   "metadata": {
    "id": "7uM9-3bIEO_Z"
   },
   "id": "7uM9-3bIEO_Z"
  },
  {
   "cell_type": "code",
   "source": [
    "# Se definen los nombres de las rutas (paths) de los archivos .bio\n",
    "rutas_archivos = {\n",
    "    \"train\": \"/content/drive/MyDrive/Analitica en Salud/sentences_train.csv\",\n",
    "    \"test\": \"/content/drive/MyDrive/Analitica en Salud/sentences_test.csv\",\n",
    "    \"valid\": \"/content/drive/MyDrive/Analitica en Salud/sentences_dev.csv\"\n",
    "}"
   ],
   "metadata": {
    "id": "cQtmO3mPILMu"
   },
   "id": "cQtmO3mPILMu",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# \ud83d\uddc2\ufe0f Carga de datos desde archivos CSV\n",
    "Este bloque permite cargar datos anotados en formato **CSV**, donde cada fila contiene un token, su etiqueta y el identificador de la oraci\u00f3n.\n",
    "\n",
    "# \ud83d\udd27 Funci\u00f3n cargar_csv_conll()\n",
    "Lee un archivo CSV con columnas como `Sentence #`, `Word` y `Tag`.\n",
    "\n",
    "- Completa los IDs de oraci\u00f3n que est\u00e9n vac\u00edos (NaN).\n",
    "\n",
    "- Elimina filas que no tengan token o etiqueta.\n",
    "\n",
    "- Agrupa los tokens y etiquetas por cada oraci\u00f3n.\n",
    "\n",
    "- Devuelve un diccionario con dos listas: `tokens` y `ner_tags`.\n",
    "\n",
    "# \ud83d\udd0d Detecci\u00f3n de etiquetas \u00fanicas\n",
    "A partir de las etiquetas presentes en el set de entrenamiento, se genera una lista ordenada llamada `LABELS` que contiene todas las etiquetas utilizadas en el esquema BIO.\n",
    "\n",
    "# \ud83c\udfd7\ufe0f Creaci\u00f3n del DatasetDict\n",
    "Define un esquema de datos (`features`) que especifica:\n",
    "\n",
    "- `tokens`: secuencias de cadenas (`palabras`).\n",
    "\n",
    "- `ner_tags`: secuencias de clases (`ClassLabel`) basadas en las etiquetas detectadas.\n",
    "\n",
    "Esto permite que las etiquetas en texto (`B_CANCER_CONCEPT`, `I_CANCER_CONCEPT`, `O`) se conviertan en n\u00fameros (0, 1, 2) que el modelo puede procesar.\n",
    "\n",
    "Luego, los datos de entrenamiento, validaci\u00f3n y prueba se transforman en objetos `Dataset` aplicando este esquema, y se agrupan en un `DatasetDict` listo para el entrenamiento."
   ],
   "metadata": {
    "id": "KfwM132aE1uv"
   },
   "id": "KfwM132aE1uv"
  },
  {
   "cell_type": "code",
   "source": [
    "# Funci\u00f3n para cargar y agrupar datos desde CSV tipo CoNLL\n",
    "def cargar_csv_conll(path):\n",
    "    df = pd.read_csv(path)\n",
    "\n",
    "    # Rellenar IDs de oraci\u00f3n (NaN) y eliminar filas con palabras o etiquetas vac\u00edas\n",
    "    df[\"Sentence #\"] = df[\"Sentence #\"].ffill()\n",
    "    df = df.dropna(subset=[\"Word\", \"Tag\"])\n",
    "\n",
    "    # Convertir a string expl\u00edcitamente para evitar errores con floats\n",
    "    df[\"Word\"] = df[\"Word\"].astype(str)\n",
    "    df[\"Tag\"] = df[\"Tag\"].astype(str)\n",
    "\n",
    "    # Agrupar tokens y etiquetas por oraci\u00f3n\n",
    "    grouped = df.groupby(\"Sentence #\")\n",
    "    tokens = grouped[\"Word\"].apply(list)\n",
    "    ner_tags = grouped[\"Tag\"].apply(list)\n",
    "\n",
    "    return {\"tokens\": tokens.tolist(), \"ner_tags\": ner_tags.tolist()}\n",
    "\n",
    "# Cargar datasets\n",
    "train_data = cargar_csv_conll(rutas_archivos[\"train\"])\n",
    "val_data = cargar_csv_conll(rutas_archivos[\"valid\"])\n",
    "test_data = cargar_csv_conll(rutas_archivos[\"test\"])\n",
    "\n",
    "# Detectar etiquetas \u00fanicas para el esquema BIO\n",
    "LABELS = sorted({etiqueta for lista in train_data[\"ner_tags\"] for etiqueta in lista})\n",
    "\n",
    "# Crear esquema de features\n",
    "features = Features({\n",
    "    \"tokens\": Sequence(Value(\"string\")),\n",
    "    \"ner_tags\": Sequence(ClassLabel(names=LABELS))\n",
    "})\n",
    "\n",
    "# Construir DatasetDict completo con casting a esquema\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": Dataset.from_dict(train_data).cast(features),\n",
    "    \"validation\": Dataset.from_dict(val_data).cast(features),\n",
    "    \"test\": Dataset.from_dict(test_data).cast(features)\n",
    "})\n",
    "\n",
    "# Mostrar informaci\u00f3n b\u00e1sica\n",
    "print(\"Etiquetas detectadas:\", LABELS)\n",
    "print(\"\\nEjemplo del dataset:\")\n",
    "print(dataset_dict[\"train\"][0])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202,
     "referenced_widgets": [
      "f354e10aba9247dfb073920de0222b93",
      "17f3a71f533140729899a903c47f716a",
      "7679a964dd9b4a5c96e41f32ec969344",
      "6bfacb7f1a7349f0b5df69edcfee8243",
      "6ac9cc2d4c344bf79497b6fdde26d1ba",
      "707fc124becf45c78e558fb0dd6ea2f2",
      "9283e2bed7ea415281dc422df2043eb4",
      "cd0a4ab56426463aacae2c792a9e1579",
      "ad6ed73045c644468253ac1c18e3ac58",
      "47e1f4d2df6c4263b3894446918b4767",
      "cba7323d1b084bc28191e808cb0bd8f2",
      "e6b34b0dcecb41e78d5de4cb57d5fcd0",
      "3a65e889a8fb445cb41a21e82e0119eb",
      "be9b03affecf484bac95636f30628257",
      "00393882625749a0b22cff55cefbaac3",
      "f465e0ccd7494dc19afe264e8382eed0",
      "0d5de4bce39c462e8737794ac16a9e84",
      "b711297f5b4a43709daa5d53cc13028a",
      "443bde9d1e7843afb5146b92cdb25ead",
      "896e8764e08b418e83a980031decda6e",
      "e6ea9155cdba48f18db267aa29abbccf",
      "00390f38445f4886bb46a9d7c9d48491",
      "b66c5badfe6b4f5bb1f432985ece6f85",
      "02ba6e1c154a459a9e89a4fb88ed640f",
      "6578adc50af5471c9735f83bfbd2fe97",
      "e1f8cc35385940819ec41453cf6b37aa",
      "ff3edd2c552746878539a2ccbd8d9f91",
      "36fe768a5f104839afcef9c4b776395f",
      "2801e05d994d4646a60417cf2882f800",
      "e54265766c89468f804324dba59fc02e",
      "93323e5f0d3648289acea0cf2c616243",
      "d636cdf178034fddbdc0761a43332aa3",
      "14027514cbdb4729b3056445538c110d"
     ]
    },
    "id": "dn65Y3CUs-zp",
    "outputId": "66c2b495-83bd-4b68-9026-7e3e7a824e7f"
   },
   "id": "dn65Y3CUs-zp",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Casting the dataset:   0%|          | 0/9788 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f354e10aba9247dfb073920de0222b93"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Casting the dataset:   0%|          | 0/2758 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e6b34b0dcecb41e78d5de4cb57d5fcd0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Casting the dataset:   0%|          | 0/2496 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b66c5badfe6b4f5bb1f432985ece6f85"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Etiquetas detectadas: ['B_CANCER_CONCEPT', 'B_CHEMOTHERAPY', 'B_DATE', 'B_DRUG', 'B_FAMILY', 'B_FREQ', 'B_IMPLICIT_DATE', 'B_INTERVAL', 'B_METRIC', 'B_OCURRENCE_EVENT', 'B_QUANTITY', 'B_RADIOTHERAPY', 'B_SMOKER_STATUS', 'B_STAGE', 'B_SURGERY', 'B_TNM', 'I_CANCER_CONCEPT', 'I_DATE', 'I_DRUG', 'I_FAMILY', 'I_FREQ', 'I_IMPLICIT_DATE', 'I_INTERVAL', 'I_METRIC', 'I_OCURRENCE_EVENT', 'I_SMOKER_STATUS', 'I_STAGE', 'I_SURGERY', 'I_TNM', 'O']\n",
      "\n",
      "Ejemplo del dataset:\n",
      "{'tokens': ['Abuela', 'materna', 'con', 'cancer', 'de', 'mama', 'a', 'los', '70', 'a\u00f1os', '.'], 'ner_tags': [4, 19, 29, 0, 16, 16, 29, 29, 10, 8, 29]}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fe9019-f67f-416b-aa7f-53bc0e7675ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6fe9019-f67f-416b-aa7f-53bc0e7675ad",
    "outputId": "6e358c3f-0cb9-4c62-8a0c-3b5811044a63"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 9788\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 2758\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokens', 'ner_tags'],\n",
       "        num_rows: 2496\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#\u2699\ufe0f Configuraci\u00f3n del modelo\n",
    "Se definen los par\u00e1metros principales del modelo:\n",
    "\n",
    "- Tarea: ner (Reconocimiento de Entidades Nombradas).\n",
    "\n",
    "- Modelo preentrenado: xlm-roberta-base, adecuado para texto en espa\u00f1ol.\n",
    "\n",
    "- Tama\u00f1o de batch: 32, utilizado durante el entrenamiento y evaluaci\u00f3n.\n",
    "\n"
   ],
   "metadata": {
    "id": "jpKBvrRZC5E1"
   },
   "id": "jpKBvrRZC5E1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d867c6bf-1ef7-4c17-bb14-465a76d23c62",
   "metadata": {
    "id": "d867c6bf-1ef7-4c17-bb14-465a76d23c62"
   },
   "outputs": [],
   "source": [
    "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
    "model_checkpoint = \"xlm-roberta-base\"\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "x = dataset_dict[\"train\"].features[f\"{task}_tags\"].feature.names\n",
    "print(x)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YjFOk2Sl2gVK",
    "outputId": "0435bb42-a757-469d-d459-07a8d9d86ec7"
   },
   "id": "YjFOk2Sl2gVK",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['B_CANCER_CONCEPT', 'B_CHEMOTHERAPY', 'B_DATE', 'B_DRUG', 'B_FAMILY', 'B_FREQ', 'B_IMPLICIT_DATE', 'B_INTERVAL', 'B_METRIC', 'B_OCURRENCE_EVENT', 'B_QUANTITY', 'B_RADIOTHERAPY', 'B_SMOKER_STATUS', 'B_STAGE', 'B_SURGERY', 'B_TNM', 'I_CANCER_CONCEPT', 'I_DATE', 'I_DRUG', 'I_FAMILY', 'I_FREQ', 'I_IMPLICIT_DATE', 'I_INTERVAL', 'I_METRIC', 'I_OCURRENCE_EVENT', 'I_SMOKER_STATUS', 'I_STAGE', 'I_SURGERY', 'I_TNM', 'O']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# \ud83d\udd27 Correcci\u00f3n y actualizaci\u00f3n de las etiquetas\n",
    "Se realiza una correcci\u00f3n en las etiquetas para pasar del formato con gui\u00f3n bajo (`B_CANCER_CONCEPT`) al formato est\u00e1ndar con gui\u00f3n medio (`B-CANCER_CONCEPT`), utilizado en el esquema BIO.\n",
    "\n",
    "Luego, se redefine el esquema de datos (`features`) con las etiquetas corregidas y se aplica a todas las particiones del dataset (`train`, `validation` y `test`). Esto garantiza que las etiquetas est\u00e9n correctamente mapeadas y que el modelo las interprete de manera adecuada."
   ],
   "metadata": {
    "id": "njheaqhBvjy9"
   },
   "id": "njheaqhBvjy9"
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import ClassLabel, Sequence, Value, Features\n",
    "\n",
    "etiquetas_corregidas = [\n",
    "    'B-CANCER_CONCEPT', 'B-CHEMOTHERAPY', 'B-DATE', 'B-DRUG', 'B-FAMILY',\n",
    "    'B-FREQ', 'B-IMPLICIT_DATE', 'B-INTERVAL', 'B-METRIC', 'B-OCURRENCE_EVENT',\n",
    "    'B-QUANTITY', 'B-RADIOTHERAPY', 'B-SMOKER_STATUS', 'B-STAGE', 'B-SURGERY', 'B-TNM',\n",
    "    'I-CANCER_CONCEPT', 'I-DATE', 'I-DRUG', 'I-FAMILY', 'I-FREQ',\n",
    "    'I-IMPLICIT_DATE', 'I-INTERVAL', 'I-METRIC', 'I-OCURRENCE_EVENT',\n",
    "    'I-SMOKER_STATUS', 'I-STAGE', 'I-SURGERY', 'I-TNM', 'O'\n",
    "]\n",
    "\n",
    "\n",
    "# Redefinir las features con etiquetas corregidas\n",
    "features_corregidas = Features({\n",
    "    \"tokens\": Sequence(Value(\"string\")),\n",
    "    f\"{task}_tags\": Sequence(ClassLabel(names=etiquetas_corregidas))\n",
    "})\n",
    "\n",
    "# Recast todos los splits\n",
    "for split in dataset_dict:\n",
    "    dataset_dict[split] = dataset_dict[split].cast(features_corregidas)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "d8b27b7223a94d4ba1650f5870dde2b8",
      "d11fa9f261a4483aa8754b34327b8b51",
      "3d46ee76f44c43f39093cd34c5c812b5",
      "ccda54e39f7c4ae7b3b3e26232b31f6b",
      "d4518adbd3464ea4bd8bcaaf7478e0c6",
      "6efddfa62e7b422dad5cc893d8e62036",
      "df6988e4c7f64d6db83667aa86b8a8d3",
      "1b4b1cdf2ca047fa8af8e64775878f4d",
      "dea5a90f91d540b5991daaec65321b7a",
      "05983eb4f21349d38b19880d929d1826",
      "3224d03020f343b69100ddd40d83bd1f",
      "ea3819bca8a04174b18d2545f60d9247",
      "fe4057c480524f2eb795f710490d788c",
      "5d18f4b7765d4687a9ad6c240d1a0682",
      "1d4738d662a0428dad4dff783e2d99df",
      "f9f493638ccf4a38bb5782aa639180ba",
      "d8ddd305950f48cd95695c847b2a012b",
      "6a213e90833742bfa9dacbbb44c5ba3f",
      "0472564dda474de09fc58a904769f8d2",
      "04e11769d7c24eedb272526aa06a9a9b",
      "2a2d9f56a6a34735810abb1393bd83d6",
      "73ca653786404e4a942d5097fba29407",
      "e24a0848095e4cb2bb6f80aed19a462d",
      "11f8381721114a119270d8b226795023",
      "9e8f899fd1af4732a9f24108c2379397",
      "e050dd6992464a508744da28ccdff40b",
      "3d2da4bd6c1d4d108081aecc9821cdfd",
      "67f2229c545b4107bd5be880171ecc45",
      "0f278ab3e1b64a4daac56599795afba3",
      "922aad7fd155453086d0d0074c7bbd3d",
      "d8583ae544f84c7da5abc933dd3b0a8f",
      "0daed10b3bdb48108eaf2acbb0ecf96d",
      "a9b43eadcf464807a6a170052943d1bd"
     ]
    },
    "id": "ZfIg-Yhm0Ccv",
    "outputId": "b6aae9b3-607a-4c8c-fe83-a7d00a4ae4c5"
   },
   "id": "ZfIg-Yhm0Ccv",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Casting the dataset:   0%|          | 0/9788 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d8b27b7223a94d4ba1650f5870dde2b8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Casting the dataset:   0%|          | 0/2758 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ea3819bca8a04174b18d2545f60d9247"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Casting the dataset:   0%|          | 0/2496 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e24a0848095e4cb2bb6f80aed19a462d"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "x = dataset_dict[\"train\"].features[f\"{task}_tags\"].feature.names\n",
    "print(x)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TaHwGZnapyx7",
    "outputId": "fb7e97f6-bc6b-451b-d132-2eb8680ed2f7"
   },
   "id": "TaHwGZnapyx7",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['B-CANCER_CONCEPT', 'B-CHEMOTHERAPY', 'B-DATE', 'B-DRUG', 'B-FAMILY', 'B-FREQ', 'B-IMPLICIT_DATE', 'B-INTERVAL', 'B-METRIC', 'B-OCURRENCE_EVENT', 'B-QUANTITY', 'B-RADIOTHERAPY', 'B-SMOKER_STATUS', 'B-STAGE', 'B-SURGERY', 'B-TNM', 'I-CANCER_CONCEPT', 'I-DATE', 'I-DRUG', 'I-FAMILY', 'I-FREQ', 'I-IMPLICIT_DATE', 'I-INTERVAL', 'I-METRIC', 'I-OCURRENCE_EVENT', 'I-SMOKER_STATUS', 'I-STAGE', 'I-SURGERY', 'I-TNM', 'O']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# \ud83d\udd17 Tokenizaci\u00f3n y alineaci\u00f3n de etiquetas\n",
    "Se utiliza el tokenizador del modelo **xlm-roberta-base**.\n",
    "\n",
    "Dado que algunas palabras pueden dividirse en subtokens, se implementa la funci\u00f3n `tokenize_and_align_labels`, que:\n",
    "\n",
    "Asigna las etiquetas solo al primer subtoken de cada palabra.\n",
    "\n",
    "Marca los subtokens adicionales con -100 para que sean ignorados durante el entrenamiento.\n",
    "\n",
    "Este proceso se aplica a todas las particiones del dataset mediante `.map()`, dejando el dataset tokenizado y listo para el entrenamiento del modelo.\n",
    "\n"
   ],
   "metadata": {
    "id": "JJV96cUMBJCx"
   },
   "id": "JJV96cUMBJCx"
  },
  {
   "cell_type": "code",
   "source": [
    "# --------------------------------------------------------------------------\n",
    "# Tokenizador + alineaci\u00f3n de etiquetas con B/I/O correctas\n",
    "# --------------------------------------------------------------------------\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"xlm-roberta-base\",\n",
    "    add_prefix_space=True          # evita la fragmentaci\u00f3n de la 1.\u00aa palabra\n",
    ")\n",
    "\n",
    "# Obtiene la lista completa de etiquetas desde el dataset\n",
    "label_list = dataset_dict[\"train\"].features[\"ner_tags\"].feature.names\n",
    "label2id   = {l: i for i, l in enumerate(label_list)}\n",
    "id2label   = {i: l for i, l in enumerate(label_list)}\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tok_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    aligned = []\n",
    "    for i, word_labels in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tok_inputs.word_ids(batch_index=i)\n",
    "        prev_wid = None\n",
    "        label_ids = []\n",
    "        for wid in word_ids:\n",
    "            if wid is None:                     # tokens especiales <s>, </s>, padding\n",
    "                label_ids.append(-100)\n",
    "            elif wid != prev_wid:               # primer sub-token de la palabra\n",
    "                label_ids.append(word_labels[wid])          # B-XXX u O\n",
    "            else:                               # sub-token interior\n",
    "                lab_name = label_list[word_labels[wid]]     # p. ej. \"B-CANCER_CONCEPT\"\n",
    "                if lab_name.startswith(\"B-\"):\n",
    "                    inside_name = \"I-\" + lab_name[2:]\n",
    "                    inside_id   = label2id.get(inside_name, word_labels[wid])\n",
    "                    label_ids.append(inside_id)             # I-XXX\n",
    "                else:                       # si la palabra es \"O\", se queda en O\n",
    "                    label_ids.append(word_labels[wid])\n",
    "            prev_wid = wid\n",
    "        aligned.append(label_ids)\n",
    "\n",
    "    tok_inputs[\"labels\"] = aligned\n",
    "    return tok_inputs\n",
    "\n",
    "# Re-mapear el dataset\n",
    "tokenized_datasets = dataset_dict.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=dataset_dict[\"train\"].column_names     # opcional, limpia columnas viejas\n",
    ")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "067446d1c53548a2a17167690335444a",
      "b0de6753624242d1ba60539757de55ff",
      "a5a4f403aa224e47a4bdc3ceed9e9742",
      "2a99d1c6b656461e8cafeee39e95c8be",
      "bfda6d1dafc54664ae13af96f93286ff",
      "5e8f6d8c1c234c1ba2a4cef7899f93ce",
      "8fb03ea624184a7e9ad77d2de26448b6",
      "04c967d15a35480dbd0cdc6c1741f4d4",
      "df936a0273d8415daf663c38c7005585",
      "765dea9d1f784b22b5b6956d7d984a2e",
      "4060e8a4baa14ec69bae26e427d5189a",
      "b2dae0cea8cd433ab1e298f82fbd302e",
      "58c2ce213a774c848e331793d63bfc43",
      "2ec067cc98e9405e9eaa4b16f1ae436c",
      "3b7f8a5df4984e7cbcfd9a67855ea1c2",
      "5d78ac5db0e74f0694d080acd2eb3065",
      "960678368c814d3f8f9b2e5f051a9ea6",
      "f9d7b601773f4609a022343dae3a6a96",
      "380f191bf1314ec9ac8cd5c2a8284d57",
      "4abe24ed01364b2cae3fe8752a468596",
      "2cf62043d5524ba8826ac9cdf3808d37",
      "81c33229d125458991438a31cda23a84",
      "6e0c14eb57d647b595a9a65a30ef862f",
      "621bbd95a54d488ba5c1fa80776c6643",
      "d84019ebbf58484cb2eb1a56f2188fd4",
      "408f5383226c418ea8ac3ddf93aa198b",
      "0818381bbd114fd389fcc995fbf24dc3",
      "b2413c41d39248c78d27fd78c0bc2729",
      "93165ed409624caabff870c587b59157",
      "0e66fdfd7ffb4ae096579b319424b968",
      "3fd36fbab6aa4d25ac05905ffb5a1e78",
      "5272ab34003c4883ad0c72292e3d7f62",
      "551270f5a6b74dfc8049a6597d404f7a"
     ]
    },
    "id": "w9eldJ6Qsyz5",
    "outputId": "7259572f-e447-4f19-b348-83b2ae3fa058"
   },
   "id": "w9eldJ6Qsyz5",
   "execution_count": null,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/9788 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "067446d1c53548a2a17167690335444a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2758 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b2dae0cea8cd433ab1e298f82fbd302e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2496 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6e0c14eb57d647b595a9a65a30ef862f"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de1fdc6-981b-4a36-acb8-de0427b90170",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5de1fdc6-981b-4a36-acb8-de0427b90170",
    "outputId": "5932558a-b446-473c-d097-43037c74ad83"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['B-CANCER_CONCEPT',\n",
       " 'B-CHEMOTHERAPY',\n",
       " 'B-DATE',\n",
       " 'B-DRUG',\n",
       " 'B-FAMILY',\n",
       " 'B-FREQ',\n",
       " 'B-IMPLICIT_DATE',\n",
       " 'B-INTERVAL',\n",
       " 'B-METRIC',\n",
       " 'B-OCURRENCE_EVENT',\n",
       " 'B-QUANTITY',\n",
       " 'B-RADIOTHERAPY',\n",
       " 'B-SMOKER_STATUS',\n",
       " 'B-STAGE',\n",
       " 'B-SURGERY',\n",
       " 'B-TNM',\n",
       " 'I-CANCER_CONCEPT',\n",
       " 'I-DATE',\n",
       " 'I-DRUG',\n",
       " 'I-FAMILY',\n",
       " 'I-FREQ',\n",
       " 'I-IMPLICIT_DATE',\n",
       " 'I-INTERVAL',\n",
       " 'I-METRIC',\n",
       " 'I-OCURRENCE_EVENT',\n",
       " 'I-SMOKER_STATUS',\n",
       " 'I-STAGE',\n",
       " 'I-SURGERY',\n",
       " 'I-TNM',\n",
       " 'O']"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "label_list = dataset_dict[\"train\"].features[f\"{task}_tags\"].feature.names\n",
    "label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# \ud83d\udd27 Carga del Tokenizer, Modelo y Data Collator para NER\n",
    "\n",
    "Se cargan los componentes esenciales para ajustar el modelo a la tarea de NER:\n",
    "\n",
    "- `AutoTokenizer`: convierte texto en tokens compatibles con el modelo.\n",
    "- `AutoModelForTokenClassification`: modelo para clasificaci\u00f3n de tokens, ajustado al n\u00famero de etiquetas (`num_labels`) seg\u00fan `label_list`.\n",
    "- `DataCollatorForTokenClassification`: gestiona el padding din\u00e1mico de los lotes (*batches*), asegurando que las entradas sean del mismo tama\u00f1o durante el entrenamiento.\n"
   ],
   "metadata": {
    "id": "ABDG1HROLlpR"
   },
   "id": "ABDG1HROLlpR"
  },
  {
   "cell_type": "code",
   "source": [
    "label_list = [\n",
    "    \"B-CANCER_CONCEPT\",\"B-CHEMOTHERAPY\",\"B-DATE\",\"B-DRUG\",\"B-FAMILY\",\n",
    "    \"B-FREQ\",\"B-IMPLICIT_DATE\",\"B-INTERVAL\",\"B-METRIC\",\"B-OCURRENCE_EVENT\",\n",
    "    \"B-QUANTITY\",\"B-RADIOTHERAPY\",\"B-SMOKER_STATUS\",\"B-STAGE\",\"B-SURGERY\",\"B-TNM\",\n",
    "    \"I-CANCER_CONCEPT\",\"I-DATE\",\"I-DRUG\",\"I-FAMILY\",\"I-FREQ\",\"I-IMPLICIT_DATE\",\n",
    "    \"I-INTERVAL\",\"I-METRIC\",\"I-OCURRENCE_EVENT\",\"I-SMOKER_STATUS\",\"I-STAGE\",\n",
    "    \"I-SURGERY\",\"I-TNM\",\"O\"\n",
    "]\n",
    "\n",
    "id2label = {i: l for i, l in enumerate(label_list)}\n",
    "label2id = {l: i for i, l in enumerate(label_list)}\n"
   ],
   "metadata": {
    "id": "BMywvs1OWfLF"
   },
   "id": "BMywvs1OWfLF",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1e5fa0-b5bd-4623-9d1e-4035af809db1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104,
     "referenced_widgets": [
      "f7249581e0ac4d6091b4ea6e32f4f859",
      "b7f2c6cee3cd409bb195a857085e9972",
      "9d33d013c08d41f9bb58b4f371e65229",
      "b68e299e88ec40d69f2e59cd1cf64b25",
      "f4518bb7946f4ceb93393b74800cd4f5",
      "859dd8ffb0ec4e898cca56f82a4b08bc",
      "3a18e922841348adb7a749130cdedc91",
      "520243353361416d9306805d8f85fe7e",
      "000c09b148b9450d840ff6aa985e2d9c",
      "20c43a1825ee4d25a0a4a35d5e458dd0",
      "67eaf0af9a76412b894718daae77062e"
     ]
    },
    "id": "fc1e5fa0-b5bd-4623-9d1e-4035af809db1",
    "outputId": "ff7d91ce-6c70-426c-e54e-a65bcce26738"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f7249581e0ac4d6091b4ea6e32f4f859"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57ab71a-e634-4b50-94b1-6b4aa063b809",
   "metadata": {
    "id": "a57ab71a-e634-4b50-94b1-6b4aa063b809"
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# \u2699\ufe0f Configuraci\u00f3n de los Par\u00e1metros de Entrenamiento\n",
    "\n",
    "Se definen los hiperpar\u00e1metros y configuraciones necesarias para el fine-tuning del modelo en la tarea de NER sobre datos de pr\u00f3stata.\n",
    "\n",
    "- **Nombre del experimento:** se genera din\u00e1micamente combinando el nombre del modelo base (`model_roberta_base`) y la tarea (`task`).\n",
    "- **`TrainingArguments`:** clase que gestiona los par\u00e1metros de entrenamiento:\n",
    "  - `eval_strategy=\"epoch\"`: eval\u00faa el modelo al final de cada \u00e9poca.\n",
    "  - `learning_rate=2e-5`: tasa de aprendizaje.\n",
    "  - `per_device_train_batch_size` y `per_device_eval_batch_size`: tama\u00f1o del batch para entrenamiento y evaluaci\u00f3n.\n",
    "  - `num_train_epochs=5`: n\u00famero de \u00e9pocas.\n",
    "  - `weight_decay=0.01`: regularizaci\u00f3n para evitar sobreajuste.\n",
    "  - `push_to_hub=True`: permite subir autom\u00e1ticamente el modelo al Hugging Face Hub.\n",
    "  - `hub_token`: token de autenticaci\u00f3n personal necesario para subir el modelo al Hugging Face Hub desde el entorno local.\n"
   ],
   "metadata": {
    "id": "CogElzbcNTBP"
   },
   "id": "CogElzbcNTBP"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df6495a-fd53-483c-a5a1-cd7109a9f6f5",
   "metadata": {
    "id": "3df6495a-fd53-483c-a5a1-cd7109a9f6f5"
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "model_roberta_base = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"{model_roberta_base}-finetuned-{task}-pulmon\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    "    report_to=\"none\",\n",
    "    hub_token=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3ce404-0aad-4b1d-b0e9-e83c426cfa71",
   "metadata": {
    "id": "cb3ce404-0aad-4b1d-b0e9-e83c426cfa71",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "91547aed041b4dafbb194c10850ee153",
      "15ee6531af5341599a686260316c6da5",
      "c299d61fcce8438299855a66828b1f1c",
      "22db6eadcd2644dbb6819008921d29b7",
      "65779e51fe9d4c76891e98281b9db746",
      "0534e327c15144fd8e6bb9ddfacc8a2a",
      "4ddf58beb9bd4ad7b18d8ad4fb71b847",
      "a947f9e647a64e0899c2569b16f3286c",
      "2338f1040641450fa7bf13fed9d82f52",
      "fdc1546c8a5c494a95c9ffa6f96c9431",
      "a1a09df1ebc448339799d81771dde0db"
     ]
    },
    "outputId": "af26fef0-0aa4-4c73-aeef-0aba9438e195"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "91547aed041b4dafbb194c10850ee153"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "try:\n",
    "    from datasets import load_metric  # Para versiones antiguas\n",
    "    metric = load_metric(\"seqeval\")\n",
    "except ImportError:\n",
    "    from evaluate import load  # Para versiones nuevas\n",
    "    metric = load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# \ud83d\udcca Funci\u00f3n de evaluaci\u00f3n\n",
    "Esta funci\u00f3n calcula las m\u00e9tricas de desempe\u00f1o del modelo en la tarea de NER.\n",
    "\n",
    "- Convierte las predicciones en etiquetas mediante argmax.\n",
    "\n",
    "- Filtra los tokens con la etiqueta -100 (Los tokens con la etiqueta O).\n",
    "\n",
    "- Traduce los \u00edndices de las etiquetas a sus nombres usando label_list.\n",
    "\n",
    "- Calcula precisi\u00f3n, recall, F1 y exactitud utilizando el objeto metric.\n",
    "\n",
    "La salida es un diccionario con estas m\u00e9tricas, que se usa para evaluar el modelo durante el entrenamiento y validaci\u00f3n."
   ],
   "metadata": {
    "id": "DeXpjqP2qo7o"
   },
   "id": "DeXpjqP2qo7o"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6549fe-dd9c-401c-8c73-652f55167c57",
   "metadata": {
    "id": "1e6549fe-dd9c-401c-8c73-652f55167c57"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# \ud83d\ude80 Definici\u00f3n del entrenador\n",
    "Se instancia el objeto `Trainer`, que gestiona el proceso de entrenamiento y evaluaci\u00f3n del modelo.\n",
    "\n",
    "Par\u00e1metros principales:\n",
    "- model: El modelo ajustado (XLM-RoBERTa adaptado para NER).\n",
    "\n",
    "- args: Argumentos de configuraci\u00f3n del entrenamiento (Definidos anteriormente)\n",
    "\n",
    "- train_dataset: Conjunto de datos de entrenamiento.\n",
    "\n",
    "- eval_dataset: Conjunto de datos de validaci\u00f3n.\n",
    "\n",
    "- data_collator: Funci\u00f3n que gestiona el padding din\u00e1mico durante el entrenamiento.\n",
    "\n",
    "- tokenizer: Tokenizador asociado al modelo.\n",
    "\n",
    "- compute_metrics: Funci\u00f3n para calcular las m\u00e9tricas de evaluaci\u00f3n en cada epoch.\n",
    "\n",
    "Este objeto simplifica el manejo de todo el ciclo de entrenamiento, validaci\u00f3n y evaluaci\u00f3n del modelo.\n",
    "\n"
   ],
   "metadata": {
    "id": "GEnSZSRDxLrW"
   },
   "id": "GEnSZSRDxLrW"
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ],
   "metadata": {
    "id": "d1BBk9QlAj8l"
   },
   "id": "d1BBk9QlAj8l",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb63c1e-77d2-4fb9-a0f5-953a16ffc345",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5cb63c1e-77d2-4fb9-a0f5-953a16ffc345",
    "outputId": "4cbbd1af-3e8f-4b35-fb66-e2c722af6b4a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/tmp/ipython-input-21-1966647044.py:3: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fadcb0d-8cff-475d-9c67-c6db1cc31c7f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "id": "9fadcb0d-8cff-475d-9c67-c6db1cc31c7f",
    "outputId": "511e8477-d0c5-45c9-bf81-3db7424f5986"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1530' max='1530' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1530/1530 04:34, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.118330</td>\n",
       "      <td>0.903383</td>\n",
       "      <td>0.913481</td>\n",
       "      <td>0.908404</td>\n",
       "      <td>0.970126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.430900</td>\n",
       "      <td>0.089423</td>\n",
       "      <td>0.916040</td>\n",
       "      <td>0.953722</td>\n",
       "      <td>0.934502</td>\n",
       "      <td>0.976849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.430900</td>\n",
       "      <td>0.077327</td>\n",
       "      <td>0.926428</td>\n",
       "      <td>0.955734</td>\n",
       "      <td>0.940853</td>\n",
       "      <td>0.980184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.087200</td>\n",
       "      <td>0.076385</td>\n",
       "      <td>0.927169</td>\n",
       "      <td>0.960541</td>\n",
       "      <td>0.943560</td>\n",
       "      <td>0.980441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.063800</td>\n",
       "      <td>0.077236</td>\n",
       "      <td>0.925456</td>\n",
       "      <td>0.963112</td>\n",
       "      <td>0.943909</td>\n",
       "      <td>0.980387</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1530, training_loss=0.19129380953857322, metrics={'train_runtime': 275.3787, 'train_samples_per_second': 177.719, 'train_steps_per_second': 5.556, 'total_flos': 2353944567252960.0, 'train_loss': 0.19129380953857322, 'epoch': 5.0})"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# \u2705 Conclusi\u00f3n del entrenamiento\n",
    "Los resultados del entrenamiento muestran una evoluci\u00f3n positiva del modelo a lo largo de las epochs:\n",
    "\n",
    "Desde la primera epoch, el modelo alcanza un rendimiento alto, con un **F1 de 0.92** y una **accuracy de 97.2%**, lo que indica que el modelo aprende r\u00e1pidamente patrones relevantes.\n",
    "\n",
    "A partir de la segunda epoch, se observa una mejora continua en todas las m\u00e9tricas, especialmente en **F1**, que sube hasta **0.94**.\n",
    "\n",
    "La p\u00e9rdida de validaci\u00f3n disminuye de manera consistente hasta estabilizarse en torno a **0.072\u20130.076**, lo que sugiere que el modelo generaliza correctamente y no presenta indicios significativos de sobreajuste.\n",
    "\n",
    "Las m\u00e9tricas de **precisi\u00f3n y recall** se mantienen balanceadas, lo que es ideal para tareas de NER, donde tanto la detecci\u00f3n correcta como la cobertura son importantes."
   ],
   "metadata": {
    "id": "Hvfe-Oc53Z59"
   },
   "id": "Hvfe-Oc53Z59"
  },
  {
   "cell_type": "markdown",
   "id": "6cda24f5-5329-4651-be98-92afb588708a",
   "metadata": {
    "id": "6cda24f5-5329-4651-be98-92afb588708a"
   },
   "source": [
    "Buenas pr\u00e1cticas:\n",
    "\n",
    "No uses test para tomar decisiones: Solo para la evaluaci\u00f3n final\n",
    "\n",
    "Usa validaci\u00f3n para ajustes: Early stopping, learning rate, etc.\n",
    "\n",
    "Guarda test para el final: Como si fuera datos \"reales\" que el modelo nunca ha visto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38af7bb5-af06-4337-b9cf-d8ee8f3bcde7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 159
    },
    "id": "38af7bb5-af06-4337-b9cf-d8ee8f3bcde7",
    "outputId": "45048c19-033d-4f06-86e4-175765917185"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='78' max='78' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [78/78 00:02]\n",
       "    </div>\n",
       "    "
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "==================================================\n",
      "Resultados finales en conjunto de test:\n",
      "F1-score: 0.934\n",
      "Precisi\u00f3n: 0.907\n",
      "Recall: 0.963\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "test_metrics = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Resultados finales en conjunto de test:\")\n",
    "print(f\"F1-score: {test_metrics['eval_f1']:.3f}\")\n",
    "print(f\"Precisi\u00f3n: {test_metrics['eval_precision']:.3f}\")\n",
    "print(f\"Recall: {test_metrics['eval_recall']:.3f}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# \ud83e\uddea Resultados finales en el conjunto de test\n",
    "El modelo mantiene un desempe\u00f1o consistente en datos no vistos, confirmando su capacidad de generalizaci\u00f3n:\n",
    "\n",
    "F1-score: 0.933\n",
    "\n",
    "Precisi\u00f3n: 0.911\n",
    "\n",
    "Recall: 0.957\n",
    "\n"
   ],
   "metadata": {
    "id": "UuZYu3on7x2y"
   },
   "id": "UuZYu3on7x2y"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a946a4-f5e0-4a87-a764-f4b4479a8c8b",
   "metadata": {
    "id": "d3a946a4-f5e0-4a87-a764-f4b4479a8c8b",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136,
     "referenced_widgets": [
      "ee7a817de450462183f998bd2d65b472",
      "be80d6e610a24f079646a22ef8bca409",
      "34d76b2fb613454681d3c4cf5056bbfc",
      "112107b9161b4083adedf26a3a894fac",
      "19db1a3aeca84f14adcde53aaae2fb37",
      "0098294b2bdb4cb5a0a1511fb0bb0f13",
      "d75e0c967aca487386948f497569eb0d",
      "37c66c988fe34083b17b286be91d07d6",
      "0643ad825867457490af46800132ce8a",
      "9ff44dd68ae6406b8ff49365ec02c476",
      "15b4d81c84b64338b0e1004d784e704f"
     ]
    },
    "outputId": "9f1844ef-f09d-4388-a2cb-403297a29bce"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Uploading...:   0%|          | 0.00/1.13G [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ee7a817de450462183f998bd2d65b472"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/FernandoValencia/xlm-roberta-base-finetuned-ner-pulmon/commit/bdbb0d21809b993bef491871dfc4a2c810f5cd9c', commit_message='End of training', commit_description='', oid='bdbb0d21809b993bef491871dfc4a2c810f5cd9c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/FernandoValencia/xlm-roberta-base-finetuned-ner-pulmon', endpoint='https://huggingface.co', repo_type='model', repo_id='FernandoValencia/xlm-roberta-base-finetuned-ner-pulmon'), pr_revision=None, pr_num=None)"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf15f6dc-1f0b-4244-9cf9-6aa2521c43a9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bf15f6dc-1f0b-4244-9cf9-6aa2521c43a9",
    "outputId": "cfda3f88-172b-4950-ab00-aa61ff62ef1c"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['B-CANCER_CONCEPT',\n",
       " 'B-CHEMOTHERAPY',\n",
       " 'B-DATE',\n",
       " 'B-DRUG',\n",
       " 'B-FAMILY',\n",
       " 'B-FREQ',\n",
       " 'B-IMPLICIT_DATE',\n",
       " 'B-INTERVAL',\n",
       " 'B-METRIC',\n",
       " 'B-OCURRENCE_EVENT',\n",
       " 'B-QUANTITY',\n",
       " 'B-RADIOTHERAPY',\n",
       " 'B-SMOKER_STATUS',\n",
       " 'B-STAGE',\n",
       " 'B-SURGERY',\n",
       " 'B-TNM',\n",
       " 'I-CANCER_CONCEPT',\n",
       " 'I-DATE',\n",
       " 'I-DRUG',\n",
       " 'I-FAMILY',\n",
       " 'I-FREQ',\n",
       " 'I-IMPLICIT_DATE',\n",
       " 'I-INTERVAL',\n",
       " 'I-METRIC',\n",
       " 'I-OCURRENCE_EVENT',\n",
       " 'I-SMOKER_STATUS',\n",
       " 'I-STAGE',\n",
       " 'I-SURGERY',\n",
       " 'I-TNM',\n",
       " 'O']"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "label_names =  dataset_dict[\"train\"].features[\"ner_tags\"].feature.names\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346bd3ef-e0eb-47b1-8105-2281ae2700b4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "346bd3ef-e0eb-47b1-8105-2281ae2700b4",
    "outputId": "735955d9-87e1-4c6c-dd7d-9ac1c5aa9e9f"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {}
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'CANCER_CONCEPT': {'precision': np.float64(0.9023383768913342),\n",
       "  'recall': np.float64(0.9521044992743106),\n",
       "  'f1': np.float64(0.9265536723163842),\n",
       "  'number': np.int64(689)},\n",
       " 'CHEMOTHERAPY': {'precision': np.float64(0.9249492900608519),\n",
       "  'recall': np.float64(1.0),\n",
       "  'f1': np.float64(0.9610115911485774),\n",
       "  'number': np.int64(456)},\n",
       " 'DATE': {'precision': np.float64(0.9808673469387755),\n",
       "  'recall': np.float64(0.9871630295250321),\n",
       "  'f1': np.float64(0.9840051183621241),\n",
       "  'number': np.int64(779)},\n",
       " 'DRUG': {'precision': np.float64(0.9116022099447514),\n",
       "  'recall': np.float64(0.9777777777777777),\n",
       "  'f1': np.float64(0.9435310936383131),\n",
       "  'number': np.int64(675)},\n",
       " 'FAMILY': {'precision': np.float64(0.9798657718120806),\n",
       "  'recall': np.float64(0.9931972789115646),\n",
       "  'f1': np.float64(0.9864864864864865),\n",
       "  'number': np.int64(147)},\n",
       " 'FREQ': {'precision': np.float64(0.8603351955307262),\n",
       "  'recall': np.float64(0.9565217391304348),\n",
       "  'f1': np.float64(0.9058823529411766),\n",
       "  'number': np.int64(161)},\n",
       " 'IMPLICIT_DATE': {'precision': np.float64(0.4634146341463415),\n",
       "  'recall': np.float64(0.7307692307692307),\n",
       "  'f1': np.float64(0.5671641791044776),\n",
       "  'number': np.int64(26)},\n",
       " 'INTERVAL': {'precision': np.float64(0.6),\n",
       "  'recall': np.float64(0.8571428571428571),\n",
       "  'f1': np.float64(0.7058823529411764),\n",
       "  'number': np.int64(21)},\n",
       " 'METRIC': {'precision': np.float64(0.9346230820547031),\n",
       "  'recall': np.float64(0.9589322381930184),\n",
       "  'f1': np.float64(0.9466216216216216),\n",
       "  'number': np.int64(1461)},\n",
       " 'OCURRENCE_EVENT': {'precision': np.float64(0.7737556561085973),\n",
       "  'recall': np.float64(0.8592964824120602),\n",
       "  'f1': np.float64(0.8142857142857143),\n",
       "  'number': np.int64(597)},\n",
       " 'QUANTITY': {'precision': np.float64(0.9299397920087575),\n",
       "  'recall': np.float64(0.9872167344567112),\n",
       "  'f1': np.float64(0.9577226606538894),\n",
       "  'number': np.int64(1721)},\n",
       " 'RADIOTHERAPY': {'precision': np.float64(0.8134328358208955),\n",
       "  'recall': np.float64(0.990909090909091),\n",
       "  'f1': np.float64(0.8934426229508198),\n",
       "  'number': np.int64(110)},\n",
       " 'SMOKER_STATUS': {'precision': np.float64(0.803030303030303),\n",
       "  'recall': np.float64(0.9636363636363636),\n",
       "  'f1': np.float64(0.8760330578512396),\n",
       "  'number': np.int64(55)},\n",
       " 'STAGE': {'precision': np.float64(0.9685534591194969),\n",
       "  'recall': np.float64(0.9935483870967742),\n",
       "  'f1': np.float64(0.980891719745223),\n",
       "  'number': np.int64(155)},\n",
       " 'SURGERY': {'precision': np.float64(0.7265625),\n",
       "  'recall': np.float64(0.8611111111111112),\n",
       "  'f1': np.float64(0.7881355932203391),\n",
       "  'number': np.int64(108)},\n",
       " 'TNM': {'precision': np.float64(0.9043062200956937),\n",
       "  'recall': np.float64(0.949748743718593),\n",
       "  'f1': np.float64(0.9264705882352942),\n",
       "  'number': np.int64(199)},\n",
       " 'overall_precision': np.float64(0.9074500768049155),\n",
       " 'overall_recall': np.float64(0.9631793478260869),\n",
       " 'overall_f1': np.float64(0.934484576852096),\n",
       " 'overall_accuracy': 0.976141934479807}"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"test\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_names[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# \ud83d\udcca Desempe\u00f1o por entidad\n",
    "El modelo muestra un alto desempe\u00f1o general, con un F1 global de 0.937, una precisi\u00f3n de 0.919 y un recall de 0.957, lo que indica una buena capacidad para identificar entidades correctamente.\n",
    "\n",
    "## \ud83d\udd0d An\u00e1lisis por entidad:\n",
    "Entidades con desempe\u00f1o sobresaliente, con F1 superiores al 0.95:\n",
    "\n",
    "CHEMOTHERAPY (0.98)\n",
    "\n",
    "DATE (0.98)\n",
    "\n",
    "FAMILY (0.99)\n",
    "\n",
    "METRIC (0.94)\n",
    "\n",
    "QUANTITY (0.96)\n",
    "\n",
    "STAGE (0.97)\n",
    "\n",
    "TNM (0.96)\n",
    "\n",
    "Buen desempe\u00f1o en entidades cl\u00ednicas clave como:\n",
    "\n",
    "CANCER_CONCEPT: F1 de 0.92\n",
    "\n",
    "DRUG: F1 de 0.94\n",
    "\n",
    "RADIOTHERAPY: F1 de 0.95\n",
    "\n",
    "Entidades con desempe\u00f1o aceptable pero con margen de mejora:\n",
    "\n",
    "OCURRENCE_EVENT: F1 de 0.80\n",
    "\n",
    "SURGERY: F1 de 0.86\n",
    "\n",
    "SMOKER_STATUS: F1 de 0.87\n",
    "\n",
    "Entidad con bajo desempe\u00f1o:\n",
    "\n",
    "IMPLICIT_DATE: F1 de 0.46, probablemente debido a su baja frecuencia (26 muestras) y a la mayor complejidad para identificar fechas impl\u00edcitas en texto cl\u00ednico."
   ],
   "metadata": {
    "id": "ue8wGxLz8MPh"
   },
   "id": "ue8wGxLz8MPh"
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "m_MbyID-8TzA"
   },
   "id": "m_MbyID-8TzA",
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Pytorch]",
   "language": "python",
   "name": "conda-env-Pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "colab": {
   "provenance": [],
   "gpuType": "A100",
   "machine_shape": "hm"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
